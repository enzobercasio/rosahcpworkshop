[{"id":0,"href":"/docs/lab_1_explore_rosa/","title":"Lab 1 - Explore the environment","section":"Docs","content":" The Workshop Environment You Are Using # Your environment has already been set up and with the tools you need to work with ROSA and AWS.\nYour workshop environment consists of several components which have been pre-configured and are ready to use. This includes an Amazon Web Services (AWS) account, and many other supporting resources.\nROSA is enabled on the AWS account used for this lab - and the ROSA CLI as well as AWS CLI tools are installed and configured on your bastion VM.\nValidate installed tools # You will be using the rosa, aws and oc command line tools throughout this lab.\nVerify that the rosa command line tool is installed (note that your version may be a more recent version than the output shown below):\nrosa version Sample Output\n1.2.37 Your ROSA CLI is up to date. Verify that the aws command line tool is installed:\naws --version Sample Output\naws-cli/2.15.39 Python/3.11.8 Linux/5.14.0-362.18.1.el9_3.x86_64 exe/x86_64.rhel.9 prompt/off Verify that the aws command line tool is configured correctly:\naws sts get-caller-identity Sample Output\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AIDA52VPS74UJLY4GUW7L\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;950629760808\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::950629760808:user/wkulhane@redhat.com-nhnv4\u0026#34; } Verify that the oc CLI is installed correctly\nrosa verify openshift-client Sample Output\nI: Verifying whether OpenShift command-line tool is available... I: Current OpenShift Client Version: 4.15.9 Rosa Login\nrosa login Sample Output\nI: Logged in as 'rhpds-cloud' on 'https://api.openshift.com' Note: Normally you would need to get a token from the Red Hat Console and log into ROSA using that token.\nIn this environment your token has been preconfigured for you.\nRun rosa whoami to verify your credentials:\nrosa whoami Sample Output\nAWS ARN: arn:aws:iam::950629760808:user/wkulhane@redhat.com-nhnv4 AWS Account ID: 950629760808 AWS Default Region: us-east-2 OCM API: https://api.openshift.com OCM Account Email: rhpds-admins+cloud@redhat.com OCM Account ID: 1z8a-------------------HD9l OCM Account Name: RHPDS Cloud OCM Account Username: rhpds-cloud OCM Organization External ID: 1234567890 OCM Organization ID: 1z8-------------------0ZL OCM Organization Name: Red Hat, Inc. Review Quota # You must ensure your AWS account has enough resources available to run ROSA. Use the following command to review available AWS quota:\nrosa verify quota Sample Output\nI: Validating AWS quota... I: AWS quota ok. If cluster installation fails, validate actual AWS resource usage against https://docs.openshift.com/ rosa/rosa_getting_started/rosa-required-aws-service-quotas.html See the documentation for more details regarding quotas.\nYou have now successfully reviewed your account and environment and are ready to work with ROSA and AWS.\n"},{"id":1,"href":"/docs/lab_2_cluster_creation_hcp/","title":"Lab 2 - Create a ROSA HCP Cluster","section":"Docs","content":" Deploy your cluster using a Hosted Control Plane # In this section we will deploy a ROSA cluster using Hosted Control Plane (HCP).\nIn short, with ROSA HCP you can decouple the control plane from the data plane (workers). This is a new deployment model for ROSA in which the control plane is hosted in a Red Hat owned AWS account. Therefore the control plane is no longer hosted in your AWS account thus reducing your AWS infrastructure expenses. The control plane is dedicated to each cluster and is highly available. See the documentation for more about Hosted Control Planes.\nPrerequisites # ROSA HCP requires a few things to be created before deploying the cluster:\nELB service role\nVPC - This is a \u0026ldquo;bring-your-own VPC\u0026rdquo; model (also referred to as BYO-VPC)\nOIDC configuration (and an OIDC provider with that specific configuration)\nLet’s create those first.\nEnsure ELB service role exists # Run the following to check for the role and create it if it is missing.\naws iam get-role --role-name \u0026quot;AWSServiceRoleForElasticLoadBalancing\u0026quot; || aws iam create-service-linked-role --aws-service-name \u0026quot;elasticloadbalancing.amazonaws.com\u0026quot; VPC # Set a few environment variables for your networking configuration:\nexport VPC_CIDR=10.0.0.0/16 export PUBLIC_CIDR_SUBNET=10.0.1.0/24 export PRIVATE_CIDR_SUBNET=10.0.0.0/24 export CLUSTER_NAME=rosa-${GUID} and save them in your .bashrc\necho \u0026quot;export VPC_CIDR=$VPC_CIDR\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc echo \u0026quot;export PUBLIC_CIDR_SUBNET=$PUBLIC_CIDR_SUBNET\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc echo \u0026quot;export PRIVATE_CIDR_SUBNET=$PRIVATE_CIDR_SUBNET\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc echo \u0026quot;export CLUSTER_NAME=$CLUSTER_NAME\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc Create the VPC\nexport VPC_ID=$(aws ec2 create-vpc --cidr-block $VPC_CIDR --query Vpc.VpcId --output text) echo \u0026quot;export VPC_ID=$VPC_ID\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc Create Tags\naws ec2 create-tags --resources $VPC_ID --tags Key=Name,Value=$CLUSTER_NAME Enable DNS Hostname\naws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames Now you can create the public subnet and add tags\nexport PUBLIC_SUBNET_ID=$(aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block $PUBLIC_CIDR_SUBNET --query Subnet.SubnetId --output text) echo \u0026quot;export PUBLIC_SUBNET_ID=$PUBLIC_SUBNET_ID\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc aws ec2 create-tags --resources $PUBLIC_SUBNET_ID --tags Key=Name,Value=$CLUSTER_NAME-public Create the private subnet and add tags\nexport PRIVATE_SUBNET_ID=$(aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block $PRIVATE_CIDR_SUBNET --query Subnet.SubnetId --output text) echo \u0026quot;export PRIVATE_SUBNET_ID=$PRIVATE_SUBNET_ID\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc aws ec2 create-tags --resources $PRIVATE_SUBNET_ID --tags Key=Name,Value=$CLUSTER_NAME-private Create an internet gateway for outbound traffic and attach it to the VPC\nexport IGW_ID=$(aws ec2 create-internet-gateway --query InternetGateway.InternetGatewayId --output text) echo \u0026quot;export IGW_ID=$IGW_ID\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc aws ec2 create-tags --resources $IGW_ID --tags Key=Name,Value=$CLUSTER_NAME aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $IGW_ID Create a route table for outbound traffic and associate it to the public subnet.\nexport PUBLIC_ROUTE_TABLE_ID=$(aws ec2 create-route-table --vpc-id $VPC_ID --query RouteTable.RouteTableId --output text) echo \u0026quot;export PUBLIC_ROUTE_TABLE_ID=$PUBLIC_ROUTE_TABLE_ID\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc aws ec2 create-tags --resources $PUBLIC_ROUTE_TABLE_ID --tags Key=Name,Value=$CLUSTER_NAME aws ec2 create-route --route-table-id $PUBLIC_ROUTE_TABLE_ID --destination-cidr-block 0.0.0.0/0 --gateway-id $IGW_ID aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET_ID --route-table-id $PUBLIC_ROUTE_TABLE_ID Create a NAT gateway in the public subnet for outgoing traffic from the private network.\nexport NAT_IP_ADDRESS=$(aws ec2 allocate-address --domain vpc --query AllocationId --output text) echo \u0026quot;export NAT_IP_ADDRESS=$NAT_IP_ADDRESS\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc export NAT_GATEWAY_ID=$(aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET_ID --allocation-id $NAT_IP_ADDRESS --query NatGateway.NatGatewayId --output text) echo \u0026quot;export NAT_GATEWAY_ID=$NAT_GATEWAY_ID\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc aws ec2 create-tags --resources $NAT_IP_ADDRESS --resources $NAT_GATEWAY_ID --tags Key=Name,Value=$CLUSTER_NAME Wait 10 seconds for the NAT gateway to be ready then create a route table for the private subnet to the NAT gateway.\nsleep 10 export PRIVATE_ROUTE_TABLE_ID=$(aws ec2 create-route-table --vpc-id $VPC_ID --query RouteTable.RouteTableId --output text) echo \u0026quot;export PRIVATE_ROUTE_TABLE_ID=$PRIVATE_ROUTE_TABLE_ID\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc aws ec2 create-tags --resources $PRIVATE_ROUTE_TABLE_ID $NAT_IP_ADDRESS --tags Key=Name,Value=$CLUSTER_NAME-private aws ec2 create-route --route-table-id $PRIVATE_ROUTE_TABLE_ID --destination-cidr-block 0.0.0.0/0 --gateway-id $NAT_GATEWAY_ID aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET_ID --route-table-id $PRIVATE_ROUTE_TABLE_ID See the documentation for more about VPC requirements.\nConfirm that the environment variables that you need to create the cluster are, in fact, set.\necho \u0026quot;Public Subnet: $PUBLIC_SUBNET_ID\u0026quot;; echo \u0026quot;Private Subnet: $PRIVATE_SUBNET_ID\u0026quot; Sample Output\nPublic Subnet: subnet-0faeeeb0000000000 Private Subnet: subnet-011fe340000000000 Warning: If one or both are blank, do not proceed and ask for assistance.\nOIDC Configuration # To create the OIDC configuration to be used for your cluster in this workshop, run the following command. We are opting for the automatic creation mode and Red Hat managed, as this is simpler for the workshop purposes. We are going to store the generated OIDC ID to an environment variable for later use. Notice that the following command uses the ROSA CLI to create your cluster’s unique OIDC configuration.\nexport OIDC_ID=$(rosa create oidc-config --mode auto --managed --yes -o json | jq -r '.id'); echo $OIDC_ID; echo \u0026quot;export OIDC_ID=$OIDC_ID\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc Sample Output\n23o3doeo86adgqhci4jl000000000000 Create the Cluster # As this is the first time you are deploying ROSA in this account and have not yet created the account roles, create the account-wide roles with policies, and Operator roles with policies. Since ROSA makes use of AWS Security Token Service (STS), this step creates the AWS IAM roles and policies that are needed for ROSA to interact within your account. See Account-wide IAM role and policy reference for more details if you are interested.\nRun the following command to create the account-wide roles\nrosa create account-roles --mode auto --yes Run the following command to create the cluster\nrosa create cluster --cluster-name rosa-${GUID} \\ --subnet-ids ${PUBLIC_SUBNET_ID},${PRIVATE_SUBNET_ID} \\ --hosted-cp \\ --version 4.15.34 \\ --oidc-config-id $OIDC_ID \\ --sts --mode auto --yes In about 10 minutes the control plane and API will be up, and about 5-10 minutes after, the worker nodes will be up and the cluster will be completely usable. This cluster will have a control plane across three AWS availability zones in your selected region, in a Red Hat AWS account and will also create 2 worker nodes in your AWS account.\nCheck installation status # You can run the following command to check the detailed status of the cluster\nrosa describe cluster --cluster rosa-${GUID} or, you can also watch the logs as it progresses\nrosa logs install --cluster rosa-${GUID} --watch Once the state changes to “ready” your cluster is now installed. It may take a few more minutes for the worker nodes to come online. In total this should take about 15 minutes.\nYou can continue this lab - there is a step in the next section where you will need to wait for the cluster operators to finish rolling out - but there is no need to wait at this point.\n"},{"id":2,"href":"/docs/lab_3_access_cluster/","title":"Lab 3 - Access ROSA HCP Cluster","section":"Docs","content":" Create a Local Admin User for Cluster Access # You can use built-in authentication to access your cluster immediately. This is done through the creation of a local privileged user.\nROSA makes this easy by providing a command to create a user, called cluster-admin, which has powerful cluster administration capabilities.\nFor production deployments, however, the recommended approach is to use an external identity provider to access the cluster. We will take you through an example of that in a later lab.\nRun this command to create the local admin user\nrosa create admin --cluster rosa-${GUID} Sample Output\nI: Admin account has been added to cluster \u0026#39;rosa-4fgbq\u0026#39;. I: Please securely store this generated password. If you lose this password you can delete and recreate the cluster admin user. I: To login, run the following command: oc login https://api.rosa-4fgbq.qrdf.p1.openshiftapps.com:6443 --username cluster-admin --password [clusteradminpassword] I: It may take up to 5 minutes for the account to become active. Save the login command somewhere. Also take note of the password for your cluster-admin user.\nMake sure to use the entire password when copying and pasting the command!\nSave an environment variable for your admin password (copy the password from above making sure to copy the entire password):\nNote this box will not automatically copy the command for you. You need to write the command yourself using the password from your environment.\nexport ADMIN_PASSWORD=[clusteradminpassword] Save the admin password in your .bashrc\necho \u0026quot;export ADMIN_PASSWORD=$ADMIN_PASSWORD\u0026quot; \u0026gt;\u0026gt;$HOME/.bashrc Copy the login command returned to you in the previous step and paste that into your terminal. This should log you into the cluster via the CLI so you can start using the cluster (answer y when prompted if you want to accept the certificate).\noc login https://api.rosa-4fgbq.qrdf.p1.openshiftapps.com:6443 --username cluster-admin --password $ADMIN_PASSWORD Sample Output\nThe server uses a certificate signed by an unknown authority. You can bypass the certificate check, but any data you send to the server could be intercepted by others. Use insecure connections? (y/n): y WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 100 projects, the list has been suppressed. You can list all projects with \u0026#39;oc projects\u0026#39; Using project \u0026#34;default\u0026#34;. Welcome! See \u0026#39;oc help\u0026#39; to get started. If you get an error that the Login failed (401 Unauthorized) wait a few seconds and then try again. It takes a few minutes for the cluster authentication operator to update itself after creating the cluster-admin user.\nTo check that you are logged in as the admin user you can run oc whoami\noc whoami Sample Output\ncluster-admin You can also confirm by running the following command. Only a cluster-admin user can run this without errors.\noc get pod -n openshift-ingress Sample Output\nNAME READY STATUS RESTARTS AGE router-default-7994f6fd58-8cl45 1/1 Running 0 102s router-default-7994f6fd58-z6gpm 1/1 Running 0 102s You can now access the cluster with this local admin user. Though, for production use, it is highly recommended to set up an external identity provider.\nLogin to the OpenShift Web Console # Next, let’s log in to the OpenShift Web Console. To do so, follow the below steps:\nFirst, we’ll need to grab your cluster’s web console URL. To do so, run the following command:\noc whoami --show-console Sample Output\nhttps://console-openshift-console.apps.rosa-z8ssd.ls3a.p1.openshiftapps.com/ Next, open the printed URL in a web browser\nClick on the cluster-admin identity provider.\nEnter the username (cluster-admin) and password from the previous section (use echo $ADMIN_PASSWORD to remind yourself what the password is in case you forgot).\nCongratulations! You’re now logged into the cluster and ready to move on to the workshop content.\n"},{"id":3,"href":"/docs/lab_4_cluster_upgrade/","title":"Lab 4 - Upgrade ROSA HCP Cluster","section":"Docs","content":" Introduction # Red Hat OpenShift Service on AWS (ROSA) provides fully-managed cluster upgrades. The ROSA Site Reliability Engineering (SRE) Team will monitor and manage all ROSA cluster upgrades. Customers get status emails from the SRE team before, during, and after the upgrade. These updates can be scheduled from the OpenShift Cluster Manager (OCM) or from the ROSA CLI.\nDuring ROSA upgrades, one node is upgraded at a time. This is done to ensure customer applications are not impacted during the update, when deployed in a highly-available and fault-tolerant method.\nThere are two ways to upgrade a ROSA cluster - using OpenShift Cluster Manager or using the rosa CLI. In this lab environment we do not have access to the OpenShift Cluster Manager so we will use the rosa CLI.\nUpgrade using the rosa command line interface # Remind yourself of the version of your cluster:\noc version Sample Output\nClient Version: 4.13.26 Kustomize Version: v4.5.7 Server Version: 4.13.26 Kubernetes Version: v1.26.11+7dfc52e Find out the available versions\nrosa list upgrades --cluster rosa-${GUID} Sample Output\nVERSION NOTES 4.16.14 recommended Note: Depending on when you run this command the available version list may be longer than just the next release.\nYou probably don’t want to actually upgrade the cluster right now since that may disrupt your lab environment. Luckily it is possible to schedule an update at a less inconvenient time.\nGet a date and time that is 24 hours from now:\nexport UPGRADE_DATE=$(date -d \u0026quot;+24 hours\u0026quot; '+%Y-%m-%d') export UPGRADE_TIME=$(date '+%H:%M') echo Date: ${UPGRADE_DATE}, Time: ${UPGRADE_TIME} Sample Output\nDate: 2023-04-18, Time: 19:51 Now schedule the cluster upgrade to a version shown in the list of available versions you found earlier (in the example above that is 4.13.25) - answer y when prompted if you really want to upgrade. To be clear, you will need to change the value specified in the \u0026ndash;version line below to be something from the list upgrades command you ran before.\nrosa upgrade cluster \\ -c rosa-${GUID} \\ --version 4.16.14 \\ --mode auto \\ --schedule-date ${UPGRADE_DATE} \\ --schedule-time ${UPGRADE_TIME} \\ --control-plane Sample Output\nI: Ensuring account and operator role policies for cluster \u0026#39;25216tq1cq14etbb8rl91kqg2s1u4ssj\u0026#39; are compatible with upgrade. I: Account roles/policies for cluster \u0026#39;25216tq1cq14etbb8rl91kqg2s1u4ssj\u0026#39; are already up-to-date. I: Operator roles/policies associated with the cluster \u0026#39;25216tq1cq14etbb8rl91kqg2s1u4ssj\u0026#39; are already up-to-date. I: Account and operator roles for cluster \u0026#39;rosa-bhjks\u0026#39; are compatible with upgrade ? Are you sure you want to upgrade cluster to version \u0026#39;4.13.1\u0026#39;? Yes I: Upgrade successfully scheduled for cluster \u0026#39;rosa-6n4s8\u0026#39; Congratulations! You’ve successfully scheduled an upgrade of your cluster for tomorrow at this time. While the workshop environment will be deleted before then, you now have the experience to schedule upgrades in the future.\nAdditional Resources # Red Hat OpenShift Upgrade Graph # Occasionally, you may be not be able to go directly from your current version to a desired version. In these cases, you must first upgrade your cluster from your current version, to an intermediary version, and then to your desired version. To help you navigate these decisions, you can take advantage of the Red Hat OpenShift Upgrade Graph Tool (Red Hat portal login required).\nIn this scenario to upgrade your cluster from version 4.11.0 to 4.12.15, you must first upgrade to 4.11.39, then you can upgrade to 4.12.15. The ROSA Upgrade Graph Tool helps you easily see which version you should upgrade to.\nLinks to Documentation # Upgrading ROSA clusters with STS\nScheduling individual upgrades through the OpenShift Cluster Manager console\nUnderstanding update channels and releases\nSummary # Here you learned:\nAll upgrades are monitored and managed by the ROSA SRE Team\nUse OpenShift Cluster Manager (OCM) to schedule an upgrade for your ROSA cluster\nExplore the OpenShift Upgrade Graph Tool to see available upgrade paths\n"},{"id":4,"href":"/docs/lab_5_managing_worker_nodes-copy/","title":"Lab 5 - Managing Worker Nodes","section":"Docs","content":" Introduction # When deploying your ROSA cluster, you can configure many aspects of your worker nodes, but what happens when you need to change your worker nodes after they’ve already been created? These activities include scaling the number of nodes, changing the instance type, adding labels or taints, just to name a few.\nMany of these changes are done using Machine Pools. Machine Pools ensure that a specified number of Machine replicas are running at any given time. Think of a Machine Pool as a \u0026ldquo;template\u0026rdquo; for the kinds of Machines that make up the worker nodes of your cluster. If you’d like to learn more, see the Red Hat documentation on worker node management.\nHere are some of the advantages of using ROSA Machine Pools to manage the size of your cluster\nScalability - A ROSA Machine Pool enables horizontal scaling of your cluster. It can easily add or remove worker nodes to handle the changes in workload. This flexibility ensures that your cluster can dynamically scale to meet the needs of your applications\nHigh Availability - ROSA Machine Pools supports the creation of 3 replicas of workers across different availability zones. This redundancy helps ensure high availability of applications by distributing workloads.\nInfrastructure Diversity - ROSA Machine Pools allow you to provision worker nodes of different instance types. This enables you to leverage the best kind of instance family for different workloads.\nIntegration with Cluster Autoscaler - ROSA Machine Pools seamlessly integrate with the Cluster Autoscaler feature, which automatically adjusts the number of worker nodes based on the current demand. This integration ensures efficient resource utilization by scaling the cluster up or down as needed, optimizing costs and performance.\nScaling Worker Nodes # Via the CLI # First, let’s see what Machine Pools already exist in our cluster. To do so, run the following command\nrosa list machinepools -c rosa-${GUID} Sample Output\nID AUTOSCALING REPLICAS INSTANCE TYPE LABELS TAINTS AVAILABILITY ZONE SUBNET DISK SIZE VERSION AUTOREPAIR workers No 2/2 m5.xlarge ap-southeast-1c subnet-0c6e1f11632b044a8 300 GiB 4.15.34 Yes Now, let’s take a look at the nodes inside of the ROSA cluster that have been created according to the instructions provided by the above MachineSets. To do so, run the following command:\noc get nodes Sample Output\nNAME STATUS ROLES AGE VERSION ip-10-0-0-237.ap-southeast-1.compute.internal Ready worker 14m v1.28.13+2ca1a23 ip-10-0-0-42.ap-southeast-1.compute.internal Ready worker 24s v1.28.13+2ca1a23 For this workshop, we’ve deployed your ROSA cluster with 2 total worder nodes.\nNow that we know that we have two worker nodes, let’s create a MachinePool to add a new worker node using the ROSA CLI. To do so, run the following command\nrosa create machinepool -c rosa-${GUID} --replicas 1 --name workshop --instance-type m5.xlarge Sample Output\nI: Fetching instance types I: Machine pool \u0026#39;workshop\u0026#39; created successfully on cluster \u0026#39;rosa-6n4s8\u0026#39; I: To view all machine pools, run \u0026#39;rosa list machinepools -c rosa-6n4s8\u0026#39; This command adds a single m5.xlarge instance to the first AWS availability zone in the region your cluster is deployed in.\nNow, let’s scale up our selected MachinePool from one to two machines. To do so, run the following command\nrosa update machinepool -c rosa-${GUID} --replicas 2 workshop Sample Output\nI: Updated machine pool \u0026#39;workshop\u0026#39; on cluster \u0026#39;rosa-6n4s8\u0026#39; Now that we’ve scaled the MachinePool to two machines, we can see that the node is already being created. First, let’s quickly check the output of the oc get nodes command we ran earlier\noc get nodes Sample Output\nNAME STATUS ROLES AGE VERSION ip-10-0-0-237.ap-southeast-1.compute.internal Ready worker 20m v1.28.13+2ca1a23 ip-10-0-0-42.ap-southeast-1.compute.internal Ready worker 6m53s v1.28.13+2ca1a23 ip-10-0-0-111.ap-southeast-1.compute.internal NotReady worker 0s v1.28.13+2ca1a23 ip-10-0-0-111.ap-southeast-1.compute.internal NotReady worker 0s v1.28.13+2ca1a23 Let the above command run until all nodes are in the Ready state. This means that they are ready and available to run Pods in the cluster. Hit CTRL-C to exit the oc command.\nWe don’t need these extra worker nodes for now so let’s scale the cluster back down to a total of 3 worker nodes by scaling down the \u0026ldquo;Workshop\u0026rdquo; Machine Pool. To do so, run the following command:\nrosa update machinepool -c rosa-${GUID} --replicas 1 workshop Now that we’ve scaled the MachinePool (and therefore the MachineSet) back down to one machine, we can see the change reflected in the cluster almost immediately. Let’s quickly check the output of the same command we ran before:\noc get nodes -w Sample Output\nNAME STATUS ROLES AGE VERSION ip-10-0-0-111.ap-southeast-1.compute.internal Ready worker 4m4s v1.28.13+2ca1a23 ip-10-0-0-237.ap-southeast-1.compute.internal Ready worker 25m v1.28.13+2ca1a23 ip-10-0-0-42.ap-southeast-1.compute.internal Ready worker 12m v1.28.13+2ca1a23 ip-10-0-0-74.ap-southeast-1.compute.internal Ready worker 4m3s v1.28.13+2ca1a23 ip-10-0-0-74.ap-southeast-1.compute.internal Ready,SchedulingDisabled worker 4m45s v1.28.13+2ca1a23 Congratulations! You’ve successfully scaled your cluster up and back down to two worker nodes.\nSummary # Here you learned:\nCreating a new Machine Pool for your ROSA cluster to add additional nodes to the cluster\nScaling your new Machine Pool up to add more nodes to the cluster\nScaling your Machine Pool down to remove worker nodes from the cluster\n"},{"id":5,"href":"/docs/lab_6_labeling_nodes/","title":"Lab 6 - Labeling Worker Nodes","section":"Docs","content":" Introduction # Labels are a useful way to select which nodes that an application will run on. These nodes are created by machines which are defined by the MachineSets we worked with in previous sections of this workshop. An example of this would be running a memory intensive application only on a specific node type.\nWhile you can directly add a label to a node, it is not recommended because nodes can be recreated, which would cause the label to disappear. Therefore we need to label the Machine pool itself.\nSet a label for the Machine Pool # Just like the last section, let’s use the workshop machine pool to add our label. We will add the label \u0026ldquo;tier=frontend\u0026rdquo; to nodes in this machine pool. To do so, run the following command\nrosa edit machinepool -c rosa-${GUID} --labels tier=frontend workshop Sample Output\nI: Updated machine pool \u0026#39;workshop\u0026#39; on cluster \u0026#39;rosa-6n4s8\u0026#39; Now, let’s verify the nodes are properly labeled. To do so, run the following command\noc get nodes --selector='tier=frontend' -o name Sample Output\nnode/ip-10-0-0-165.ap-southeast-1.compute.internal node/ip-10-0-0-172.ap-southeast-1.compute.internal You may see between one and two nodes - depending if you did the scaling lab recently (if you did the cluster has probably not scaled down yet). This demonstrates that our machine pool and associated nodes are properly annotated!\nDeploy an app to the labeled nodes # Now that we’ve successfully labeled our nodes, let’s deploy a workload to demonstrate app placement using nodeSelector. This should force our app to only deploy on our labeled nodes.\nFirst, let’s create a project (or namespace) for our application. To do so, run the following command\noc new-project nodeselector-ex Sample Output\nNow using project \u0026#34;nodeselector-ex\u0026#34; on server \u0026#34;https://api.rosa-6n4s8.1c1c.p1.openshiftapps.com:6443\u0026#34;. You can add applications to this project with the \u0026#39;new-app\u0026#39; command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname Next, let’s deploy our application and associated resources that will target our labeled nodes. To do so, run the following command:\ncat \u0026laquo; EOF | oc apply -f - kind: Deployment apiVersion: apps/v1 metadata: name: nodeselector-app namespace: nodeselector-ex spec: replicas: 1 selector: matchLabels: app: nodeselector-app template: metadata: labels: app: nodeselector-app spec: nodeSelector: tier: frontend containers: - name: hello-openshift image: \u0026ldquo;docker.io/openshift/hello-openshift\u0026rdquo; ports: - containerPort: 8080 protocol: TCP - containerPort: 8888 protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL EOF\nSample Output\ndeployment.apps/nodeselector-app created Double check the name of the node to compare it to the output above to ensure the node selector worked to put the pod on the correct node.\noc get nodes --selector='tier=frontend' -o name Sample Output\nnode/ip-10-0-133-248.us-east-2.compute.internal node/ip-10-0-146-31.us-east-2.compute.internal Next create a service using the oc expose command\noc expose deployment nodeselector-app Sample Output\nservice/nodeselector-app exposed Expose the newly created service with a route\noc create route edge --service=nodeselector-app --insecure-policy=Redirect Sample Output\nroute.route.openshift.io/nodeselector-app created Fetch the URL for the newly created route\noc get routes/nodeselector-app -o json | jq -r '.spec.host' Then visit the URL presented in a new tab in your web browser.\nNote that the application is exposed over the default ingress using a predetermined URL and trusted TLS certificate. This is done using the OpenShift Route resource which is an extension to the Kubernetes Ingress resource.\nNow let’s scale the cluster back down to a total of 2 worker nodes by deleting the \u0026ldquo;Workshop\u0026rdquo; Machine Pool. To do so, run the following command:\nrosa delete machinepool -c rosa-${GUID} workshop --yes Sample Output\nI: Successfully deleted machine pool \u0026#39;workshop\u0026#39; from cluster \u0026#39;rosa-6n4s8\u0026#39; You can validate that the MachinePool has been deleted by using the rosa cli\nrosa list machinepools -c rosa-${GUID} Congratulations! You’ve successfully demonstrated the ability to label nodes and target those nodes using nodeSelector.\nSummary # Here you learned:\nAdd labels to Machine Pools\nDeploy an application on nodes with certain labels using nodeSelector\n"},{"id":6,"href":"/docs/lab_7_autoscaling/","title":"Lab 7 - Autoscaling ROSA HCP Cluster","section":"Docs","content":" Introduction # The ROSA Cluster Autoscaler is a feature that helps automatically adjust the size of an ROSA cluster based on the current workload and resource demands. Cluster Autoscaler offers automatic and intelligent scaling of ROSA clusters, leading to efficient resource utilization, improved application performance, high availability, and simplified cluster management. By dynamically adjusting the cluster size based on workload demands, it helps organizations optimize their infrastructure costs while ensuring optimal application performance and scalability. The cluster autoscaler does not increase the cluster resources beyond the limits that you specify.\nEnable Autoscaling on the Default MachinePool # You can enable autoscaling on your cluster using either the rosa CLI or the Red Hat OpenShift Cluster Manager. For this lab we will be using the CLI; however, there are instructions at the end of the lab showing how to do it in the console.\nYou will need to set up autoscaling for each MachinePool in the cluster separately.\nTo identify the machine pool IDs in a cluster, enter the following command\nrosa list machinepools --cluster rosa-${GUID} Sample Output\nID AUTOSCALING REPLICAS INSTANCE TYPE LABELS TAINTS AVAILABILITY ZONES SUBNETS SPOT INSTANCES DISK SIZE SG IDs worker No 2 m5.xlarge us-east-2a The ID of the MachinePool that you want to add autoscaling to is worker.\nTo enable autoscaling on a machine pool, enter the following command\nrosa edit machinepool --cluster rosa-${GUID} workers --enable-autoscaling --min-replicas=2 --max-replicas=4 Sample Output\nI: Updated machine pool \u0026#39;workes\u0026#39; on cluster \u0026#39;rosa-6n4s8\u0026#39; Next, let’s check to see that our managed machine autoscalers have been created. To do so, run the following command\noc -n openshift-machine-api get machineautoscaler Sample Output\nNAME REF KIND REF NAME MIN MAX AGE rosa-6n4s8-7hbhw-worker-us-east-2a MachineSet rosa-6n4s8-7hbhw-worker-us-east-2a 2 4 4m30s And finally, let’s check to see that our cluster autoscaler has been created. To do so, run the following command\noc get clusterautoscaler Sample Output\nNAME AGE default 4m55s Test the Autoscaler # Now let’s test the cluster autoscaler and see it in action. To do so, we’ll deploy a job with a load that this cluster cannot handle. This should force the cluster to scale to handle the load.\nFirst, let’s create a namespace (also known as a project in OpenShift). To do so, run the following command\noc new-project autoscale-ex Sample Output\nNow using project \u0026#34;autoscale-ex\u0026#34; on server \u0026#34;https://api.rosa-6n4s8.1c1c.p1.openshiftapps.com:6443\u0026#34;. You can add applications to this project with the \u0026#39;new-app\u0026#39; command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname Next, let’s deploy our job that will exhaust the cluster’s resources and cause it to scale more worker nodes. To do so, run the following command\ncat \u0026lt;\u0026lt; EOF | oc apply -f - --- apiVersion: batch/v1 kind: Job metadata: name: maxscale namespace: autoscale-ex spec: template: spec: containers: - name: work image: busybox command: [\u0026quot;sleep\u0026quot;, \u0026quot;300\u0026quot;] resources: requests: memory: 500Mi cpu: 500m securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL restartPolicy: Never backoffLimit: 4 completions: 50 parallelism: 50 EOF Sample Output\njob.batch/maxscale created After a few seconds, run the following to see what pods have been created.\noc -n autoscale-ex get pods Sample Output\nNAME READY STATUS RESTARTS AGE maxscale-2c6zt 1/1 Running 0 29s maxscale-2ps5g 0/1 Pending 0 29s maxscale-42l2d 0/1 Pending 0 29s maxscale-4n8rt 0/1 Pending 0 29s maxscale-5888n 1/1 Running 0 29s [...Output Omitted...] Notice that we see a lot of pods in a pending state. This should trigger the cluster autoscaler to create more machines using the MachineAutoscaler we created.\nLet’s check to see if our MachineSet automatically scaled (it may take a few minutes). To do so, run the following command\noc -n openshift-machine-api get machinesets Sample Output\nNAME DESIRED CURRENT READY AVAILABLE AGE rosa-6n4s8-7hbhw-infra-us-east-2a 2 2 2 2 23h rosa-6n4s8-7hbhw-worker-us-east-2a 4 4 2 2 23h This shows that the cluster autoscaler is working on scaling multiple MachineSets up to 4.\nNow let’s watch the cluster autoscaler create and delete machines as necessary (it may take several minutes for machines to appear in the Running state). To do so, run the following command\nwatch oc -n openshift-machine-api get machines -l machine.openshift.io/cluster-api-machine-role=worker Sample Output\nNAME PHASE TYPE REGION\tZONE AGE rosa-6n4s8-7hbhw-worker-us-east-2a-vpfqr Provisioned m5.xlarge us-east-2 us-east-2a 99s rosa-6n4s8-7hbhw-worker-us-east-2a-wwmj7 Provisioned m5.xlarge us-east-2 us-east-2a 99s rosa-6n4s8-7hbhw-worker-us-east-2a-xc8g2 Running\tm5.xlarge us-east-2 us-east-2a 23h rosa-6n4s8-7hbhw-worker-us-east-2a-zxm8j Running\tm5.xlarge us-east-2 us-east-2a 23h Tip: Watch will refresh the output of a command every second. Hit CTRL and c on your keyboard to exit the watch command when you’re ready to move on to the next part of the workshop.\nOnce the machines are running stop the watch and re-run the command to display the pods for the job. You should see that more pods are now running. If you still see some pods in Pending state that is normal because even 4 worker nodes may not be enough to handle the node - but you limited the autoscaler to 4 worker nodes.\noc -n autoscale-ex get pods Sample Output\nNAME READY STATUS RESTARTS AGE maxscale-2c6zt 0/1 Completed 0 5m18s maxscale-2ps5g 0/1 ContainerCreating 0 5m18s maxscale-42l2d 0/1 ContainerCreating 0 5m18s maxscale-4n8rt 0/1 Pending 0 5m18s maxscale-5888n 0/1 Completed 0 5m18s maxscale-5944p 0/1 Completed 0 5m18s maxscale-5nwfz 0/1 Pending 0 5m18s maxscale-5p2n8 0/1 ContainerCreating 0 5m18s [...Output omitted...] Congratulations! You’ve successfully demonstrated cluster autoscaling.\nSummary # Here you learned:\nEnable autoscaling on the default Machine Pool for your cluster\nDeploy an application on the cluster and watch the cluster autoscaler scale your cluster to support the increased workload\n"},{"id":7,"href":"/docs/lab_8_cloudwatch/","title":"Lab 8 - Configure Red Hat OpenShift Logging with AWS Cloudwatch","section":"Docs","content":" Introduction # Red Hat OpenShift Service on AWS (ROSA) clusters store log data inside the cluster by default. Understanding metrics and logs is critical in successfully running your cluster. Included with ROSA is the OpenShift Cluster Logging Operator, which is intended to simplify log management and analysis within a ROSA cluster, offering centralized log collection, powerful search capabilities, visualization tools, and integration with other monitoring systems like Amazon CloudWatch.\nAmazon CloudWatch is a monitoring and observability service provided by Amazon Web Services. It allows you to collect, store, analyze and visualize logs, metrics and events from various AWS resources and applications. Since ROSA is a first party AWS service, it integrates with Amazon CloudWatch and forwards its infrastructure, audit and application logs to Amazon CloudWatch.\nIn this section of the workshop, we’ll configure ROSA to forward logs to Amazon CloudWatch.\nPrepare Amazon CloudWatch # First, let’s set some helper variables that we’ll need throughout this section of the workshop. To do so, run the following command\nexport OIDC_ENDPOINT=$(oc get authentication.config.openshift.io \\ cluster -o json | jq -r .spec.serviceAccountIssuer | \\ sed 's|^https://||') export AWS_ACCOUNT_ID=$(aws sts get-caller-identity \\ --query Account --output text) echo OIDC_Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID} Sample Output\nOIDC_Endpoint: rh-oidc.s3.us-east-1.amazonaws.com/235ftpmaq3oavfin8mt600af4sar9oej, AWS Account ID: 264091519843 Validate that a policy RosaCloudWatch already exists and if it does not create it\nPOLICY_ARN=$(aws iam list-policies --query \u0026quot;Policies[?PolicyName=='RosaCloudWatch'].{ARN:Arn}\u0026quot; --output text) if [[ -z \u0026quot;${POLICY_ARN}\u0026quot; ]]; then cat \u0026lt;\u0026lt; EOF \u0026gt; ${HOME}/policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:DescribeLogGroups\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot;, \u0026quot;logs:PutRetentionPolicy\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:logs:*:*:*\u0026quot; } ] } EOF POLICY_ARN=$(aws iam create-policy --policy-name \u0026quot;RosaCloudWatch\u0026quot; \\ --policy-document file:///${HOME}/policy.json --query Policy.Arn --output text) fi echo ${POLICY_ARN} Sample Output\nI: Updated machine pool \u0026#39;worker\u0026#39; on cluster \u0026#39;rosa-6n4s8\u0026#39; Next, let’s create a trust policy document which will define what service account can assume our role. To create the trust policy document, run the following command\ncat \u0026lt;\u0026lt;EOF \u0026gt; ${HOME}/cloudwatch-trust-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Federated\u0026quot;: \u0026quot;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_ENDPOINT}\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRoleWithWebIdentity\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;${OIDC_ENDPOINT}:sub\u0026quot;: \u0026quot;system:serviceaccount:openshift-logging:logcollector\u0026quot; } } }] } EOF Next, let’s take the trust policy document and use it to create a role. To do so, run the following command\nROLE_ARN=$(aws iam create-role --role-name \u0026quot;RosaCloudWatch-${GUID}\u0026quot; \\ --assume-role-policy-document file://${HOME}/cloudwatch-trust-policy.json \\ --tags \u0026quot;Key=rosa-workshop,Value=true\u0026quot; \\ --query Role.Arn --output text) echo ${ROLE_ARN} Sample Output\narn:aws:iam::264091519843:role/RosaCloudWatch-6n4s8 Now, let’s attach the pre-created RosaCloudWatch IAM policy to the newly created IAM role.\naws iam attach-role-policy \\ --role-name \u0026quot;RosaCloudWatch-${GUID}\u0026quot; \\ --policy-arn \u0026quot;${POLICY_ARN}\u0026quot; Configure Cluster Logging # The CLO (Cluster Logging Operator) provides a set of APIs to control collection and forwarding of logs from all pods and nodes in a cluster. This includes application logs (from regular pods), infrastructure logs (from system pods and node logs), and audit logs (special node logs with legal/security implications). In this section we will install cluster logging operator on the ROSA cluster and configure it to forward logs to Amazon CloudWatch.\nNow, we need to deploy the OpenShift Cluster Logging Operator. We’ll need a Namespace, an OperatorGroup, and a Subscription. To do so, run the following command\ncat \u0026lt;\u0026lt; EOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \u0026quot;\u0026quot; labels: openshift.io/cluster-monitoring: \u0026quot;true\u0026quot; --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/cluster-logging.openshift-logging: \u0026quot;\u0026quot; name: cluster-logging namespace: openshift-logging spec: channel: stable installPlanApproval: Automatic name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace EOF Sample Output\nsubscription.operators.coreos.com/cluster-logging created Now, we will wait for the OpenShift Cluster Logging Operator to install. To do so, we can run the following command to watch the status of the installation\noc -n openshift-logging rollout status deployment \\ cluster-logging-operator After a minute or two, your output should look something like this:\ndeployment \u0026quot;cluster-logging-operator\u0026quot; successfully rolled out Note: If you get an error Error from server (NotFound): deployments.apps \u0026ldquo;cluster-logging-operator\u0026rdquo; not found wait a few seconds and try again.\nNext, we need to create a secret containing the ARN of the IAM role that we previously created above. To do so, run the following command\ncat \u0026lt;\u0026lt; EOF | oc apply -f - --- apiVersion: v1 kind: Secret metadata: name: cloudwatch-credentials namespace: openshift-logging stringData: role_arn: ${ROLE_ARN} EOF Sample Output\nsecret/cloudwatch-credentials created Next, let’s configure the OpenShift Cluster Logging Operator by creating a Cluster Log Forwarding custom resource that will forward logs to Amazon CloudWatch. To do so, run the following command\ncat \u0026lt;\u0026lt; EOF | oc apply -f - --- apiVersion: logging.openshift.io/v1 kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputs: - name: cw type: cloudwatch cloudwatch: groupBy: namespaceName groupPrefix: rosa-${GUID} region: $(aws configure get region) secret: name: cloudwatch-credentials pipelines: - name: to-cloudwatch inputRefs: - infrastructure - audit - application outputRefs: - cw EOF Sample Output\nclusterlogforwarder.logging.openshift.io/instance created Next, let’s create a Cluster Logging custom resource which will enable the OpenShift Cluster Logging Operator to start collecting logs.\ncat \u0026lt;\u0026lt; EOF | oc apply -f - --- apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging spec: collection: logs: type: fluentd forwarder: fluentd: {} managementState: Managed EOF Sample Output\nclusterlogging.logging.openshift.io/instance created After a few minutes, you should begin to see log groups inside of Amazon CloudWatch\naws logs describe-log-groups \\ --log-group-name-prefix rosa-${GUID} Sample Output\n{ \u0026#34;logGroups\u0026#34;: [ { \u0026#34;logGroupName\u0026#34;: \u0026#34;rosa-fxxj9.audit\u0026#34;, \u0026#34;creationTime\u0026#34;: 1682098364311, \u0026#34;metricFilterCount\u0026#34;: 0, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:logs:us-east-2:511846242393:log-group:rosa-fxxj9.audit:*\u0026#34;, \u0026#34;storedBytes\u0026#34;: 0 }, { \u0026#34;logGroupName\u0026#34;: \u0026#34;rosa-fxxj9.infrastructure\u0026#34;, \u0026#34;creationTime\u0026#34;: 1682098364399, \u0026#34;metricFilterCount\u0026#34;: 0, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:logs:us-east-2:511846242393:log-group:rosa-fxxj9.infrastructure:*\u0026#34;, \u0026#34;storedBytes\u0026#34;: 0 } ] } Congratulations!\nYou’ve successfully forwarded your cluster’s logs to the Amazon CloudWatch service.\nSummary # Here you learned:\nCreate an AWS IAM trust policy and role to grant your cluster access to Amazon CloudWatch\nInstall the OpenShift Cluster Logging Operator in your cluster\nConfigure ClusterLogForwarder and ClusterLogging objects to forward infrastructure, audit and application logs to Amazon CloudWatch\n"},{"id":8,"href":"/docs/lab_9_deploy_app/","title":"Lab 9 - Deploy an application with AWS Database","section":"Docs","content":" Introduction # It’s time for us to put our cluster to work and deploy a workload! We’re going to build an example Java application, microsweeper, using Quarkus (a Kubernetes-native Java stack) and Amazon DynamoDB. We’ll then deploy the application to our ROSA cluster and connect to the database over AWS’s secure network.\nThis lab demonstrates how ROSA (an AWS native service) can easily and securely access and utilize other AWS native services using AWS Secure Token Service (STS). To achieve this, we will be using AWS IAM, Amazon DynamoDB, and a service account within OpenShift. After configuring the latter, we will use both Quarkus - a Kubernetes-native Java framework optimized for containers - and Source-to-Image (S2I) - a toolkit for building container images from source code - to deploy the microsweeper application.\nCreate an Amazon DynamoDB Instance # First, let’s create a project (also known as a namespace). A project is a unit of organization within OpenShift that provides isolation for applications and resources. To do so, run the following command\noc new-project microsweeper-ex Sample Output\nNow using project \u0026#34;microsweeper-ex\u0026#34; on server \u0026#34;https://api.rosa-6n4s8.1c1c.p1.openshiftapps.com:6443\u0026#34;. You can add applications to this project with the \u0026#39;new-app\u0026#39; command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname Next, create the Amazon DynamoDB table resource. Amazon DynamoDB will be used to store information from our application and ROSA will utilize AWS Secure Token Service (STS) to access this native service. More information on STS and how it is utilized in ROSA will be provided in the next section. For now let’s create the Amazon DynamoDB table, To do so, run the following command\naws dynamodb create-table \\ --table-name microsweeper-scores-${GUID} \\ --attribute-definitions AttributeName=name,AttributeType=S \\ --key-schema AttributeName=name,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 Sample Output\n{ \u0026#34;TableDescription\u0026#34;: { \u0026#34;AttributeDefinitions\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } ], \u0026#34;TableName\u0026#34;: \u0026#34;microsweeper-scores-6n4s8\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; } ], \u0026#34;TableStatus\u0026#34;: \u0026#34;CREATING\u0026#34;, \u0026#34;CreationDateTime\u0026#34;: 1681832377.864, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;NumberOfDecreasesToday\u0026#34;: 0, \u0026#34;ReadCapacityUnits\u0026#34;: 1, \u0026#34;WriteCapacityUnits\u0026#34;: 1 }, \u0026#34;TableSizeBytes\u0026#34;: 0, \u0026#34;ItemCount\u0026#34;: 0, \u0026#34;TableArn\u0026#34;: \u0026#34;arn:aws:dynamodb:us-east-2:264091519843:table/microsweeper-scores-6n4s8\u0026#34;, \u0026#34;TableId\u0026#34;: \u0026#34;37be72fe-3dea-411c-871d-467c12607691\u0026#34; } } IAM Roles for Service Account (IRSA) Configuration # Our application uses AWS Secure Token Service(STS) to establish connections with Amazon DynamoDB. Traditionally, one would use static IAM credentials for this purpose, but this approach goes against AWS\u0026rsquo; recommended best practices. Instead, AWS suggests utilizing their Secure Token Service (STS). Fortunately, our ROSA cluster has already been deployed using AWS STS, making it effortless to adopt IAM Roles for Service Accounts (IRSA), also known as pod identity.\nService accounts play a crucial role in managing the permissions and access control of applications running within ROSA. They act as identities for pods and allow them to interact securely with various AWS services.\nIAM roles, on the other hand, define a set of permissions that can be assumed by trusted entities within AWS. By associating an AWS IAM role with a service account, we enable the pods in our ROSA cluster to leverage the permissions defined within that role. This means that instead of relying on static IAM credentials, our application can obtain temporary security tokens from AWS STS by assuming the associated IAM role.\nThis approach aligns with AWS\u0026rsquo; recommended best practices and provides several benefits. Firstly, it enhances security by reducing the risk associated with long-lived static credentials. Secondly, it simplifies the management of access controls by leveraging IAM roles, which can be centrally managed and easily updated. Finally, it enables seamless integration with AWS services, such as DynamoDB, by granting the necessary permissions to the service accounts associated with our pods.\nFirst, create a service account to use to assume an IAM role. To do so, run the following command\noc -n microsweeper-ex create serviceaccount microsweeper Sample Output\nserviceaccount/microsweeper created Next, let’s create a trust policy document which will define what service account can assume our role. To create the trust policy document, run the following command\ncat \u0026lt;\u0026lt;EOF \u0026gt; ${HOME}/trust-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Federated\u0026quot;: \u0026quot;arn:aws:iam::$(aws sts get-caller-identity --query 'Account' --output text):oidc-provider/$(rosa describe cluster -c rosa-${GUID} -o json | jq -r .aws.sts.oidc_endpoint_url | sed -e 's/^https:\\/\\///')\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRoleWithWebIdentity\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;$(rosa describe cluster -c rosa-${GUID} -o json | jq -r .aws.sts.oidc_endpoint_url | sed -e 's/^https:\\/\\///'):sub\u0026quot;: \u0026quot;system:serviceaccount:microsweeper-ex:microsweeper\u0026quot; } } } ] } EOF Next, let’s take the trust policy document and use it to create a role. To do so, run the following command\naws iam create-role --role-name irsa-${GUID} --assume-role-policy-document file://${HOME}/trust-policy.json --description \u0026quot;IRSA Role (${GUID}\u0026quot; Sample Output\n{ \u0026#34;Role\u0026#34;: { \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;RoleName\u0026#34;: \u0026#34;irsa_6n4s8\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;AROAT27IUZNRSSYVO24ET\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::264091519843:role/irsa_6n4s8\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-04-18T18:15:48Z\u0026#34;, \u0026#34;AssumeRolePolicyDocument\u0026#34;: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::264091519843:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/235ftpmaq3oavfin8mt600af4sar9oej\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;rh-oidc.s3.us-east-1.amazonaws.com/235ftpmaq3oavfin8mt600af4sar9oej:sub\u0026#34;: \u0026#34;system:serviceaccount:microsweeper-ex:microsweeper\u0026#34; } } } ] } } } Next, let’s attach the AmazonDynamoDBFullAccess policy to our newly created IAM role. This will allow our application to read and write to our Amazon DynamoDB table. To do so, run the following command\naws iam attach-role-policy --role-name irsa-${GUID} --policy-arn=arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess Finally, let’s annotate the service account with the ARN of the IAM role we created above. To do so, run the following command\noc -n microsweeper-ex annotate serviceaccount microsweeper eks.amazonaws.com/role-arn=arn:aws:iam::$(aws sts get-caller-identity --query 'Account' --output text):role/irsa-${GUID} Sample Output\nserviceaccount/microsweeper annotated Build and Deploy the Microsweeper app # Now that we’ve got a DynamoDB instance up and running and our IRSA configuration completed, let’s build and deploy our application.\nIn order to build the application you will need the Java JDK 17 and the Quarkus CLI installed. Java JDK 17 is already installed on your bastion VM so let’s install the Quarkus CLI\ncurl -Ls https://sh.jbang.dev | bash -s - trust add https://repo1.maven.org/maven2/io/quarkus/quarkus-cli/ curl -Ls https://sh.jbang.dev | bash -s - app install --fresh --force quarkus@quarkusio echo \u0026quot;export JAVA_HOME=/usr/lib/jvm/jre-17-openjdk\u0026quot; \u0026gt;\u0026gt;${HOME}/.bashrc echo \u0026quot;export PATH=\\$JAVA_HOME/bin:\\$PATH\u0026quot; \u0026gt;\u0026gt;${HOME}/.bashrc source ${HOME}/.bashrc Double check the Quarkus CLI version\nquarkus --version Sample Output\n3.7.2 Now, let’s clone the application from GitHub. To do so, run the following command\ncd ${HOME} git clone https://github.com/rh-mobb/rosa-workshop-app.git Next, let’s change directory into the newly cloned Git repository. To do so, run the following command\ncd ${HOME}/rosa-workshop-app Next, we will add the OpenShift extension to the Quarkus CLI. To do so, run the following command\nquarkus ext add openshift Sample Output\nLooking for the newly published extensions in registry.quarkus.io 👍 Extension io.quarkus:quarkus-openshift was already installed Now, we’ll configure Quarkus to use the DynamoDB instance that we created earlier in this section. To do so, we’ll create an application.properties file using by running the following command:\ncat \u0026lt;\u0026lt;EOF \u0026gt; ${HOME}/rosa-workshop-app/src/main/resources/application.properties # AWS DynamoDB configurations %dev.quarkus.dynamodb.endpoint-override=http://localhost:8000 %prod.quarkus.openshift.env.vars.aws_region=$(aws configure get region) %prod.quarkus.dynamodb.aws.credentials.type=default dynamodb.table=microsweeper-scores-${GUID} # OpenShift configurations %prod.quarkus.kubernetes-client.trust-certs=true %prod.quarkus.kubernetes.deploy=true %prod.quarkus.kubernetes.deployment-target=openshift %prod.quarkus.openshift.build-strategy=docker %prod.quarkus.openshift.route.expose=true %prod.quarkus.openshift.service-account=microsweeper # To make Quarkus use Deployment instead of DeploymentConfig %prod.quarkus.openshift.deployment-kind=Deployment %prod.quarkus.container-image.group=microsweeper-ex EOF Now that we’ve provided the proper configuration, we will build our application. We’ll do this using source-to-image, a tool built-in to OpenShift. To start the build and deploy, run the following command:\nquarkus build --no-tests Sample Output\n[...Lots of Output Omitted...] [INFO] Installing /home/rosa/rosa-workshop-app/target/microsweeper-appservice-1.0.0-SNAPSHOT.jar to /home/rosa/.m2/repository/org/acme/microsweeper-appservice/1.0.0-SNAPSHOT/microsweeper-appservice-1.0.0-SNAPSHOT.jar [INFO] Installing /home/rosa/rosa-workshop-app/pom.xml to /home/rosa/.m2/repository/org/acme/microsweeper-appservice/1.0.0-SNAPSHOT/microsweeper-appservice-1.0.0-SNAPSHOT.pom [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 02:02 min [INFO] Finished at: 2023-04-18T18:32:26Z [INFO] ------------------------------------------------------------------------ Test the application # In the route section of the OpenShift Web Console, click the URL under Location.\nYou can also get the the URL for your application using the command line:\necho \u0026quot;http://$(oc -n microsweeper-ex get route microsweeper-appservice -o jsonpath='{.spec.host}')\u0026quot; Sample Output\nhttp://microsweeper-appservice-microsweeper-ex.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com Warning: This is an http URL. Some browsers (Chrome) replace http with https automatically which will result in a application not available error. Either use another browser or fix the URL manually in the URL bar.\nApplication IP # Let’s take a quick look at what IP the application resolves to.\nBack in your bastion VM, run the following command:\nnslookup $(oc -n microsweeper-ex get route microsweeper-appservice -o jsonpath='{.spec.host}') Sample Output\nServer:\t192.168.0.2 Address:\t192.168.0.2#53 Non-authoritative answer: Name:\tmicrosweeper-appservice-microsweeper-ex.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com Address: 54.185.165.99 Name:\tmicrosweeper-appservice-microsweeper-ex.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com Address: 54.191.151.187 Notice the IP address; can you guess where it comes from?\nIt comes from the ROSA Load Balancer. In this workshop, we are using a public cluster which means the load balancer is exposed to the Internet. If this was a private cluster, you would have to have connectivity to the VPC ROSA is running on. This could be via a VPN connection, AWS DirectConnect, or something else.\nSummary # Here you learned:\nCreate an AWS DynamoDB table for your application to use\nCreate a service account and AWS IAM resources to use IAM Roles for Service Accounts (IRSA).\nDeploy the Microsweeper app and connect it to AWS DynamoDB as the backend database\nAccess the publicly exposed Microsweeper app using OpenShift routes.\n"},{"id":9,"href":"/docs/lab_10_openshift_gitops/","title":"Lab 10 - Deploy an application with Red Hat OpenShift GitOps","section":"Docs","content":" Introduction # Red Hat® OpenShift® GitOps is an operator that provides a workflow that integrates git repositories, continuous integration/continuous delivery (CI/CD) tools, and Kubernetes to realize faster, more secure, scalable software development, without compromising quality.\nOpenShift GitOps enables customers to build and integrate declarative git driven CD workflows directly into their application development platform.\nThere’s no single tool that converts a development pipeline to \u0026ldquo;DevOps\u0026rdquo;. By implementing a GitOps framework, updates and changes are pushed through declarative code, automating infrastructure and deployment requirements, and CI/CD.\nOpenShift GitOps takes advantage of Argo CD and integrates it into Red Hat OpenShift to deliver a consistent, fully supported, declarative Kubernetes platform to configure and use with GitOps principles.\nOpenShift and OpenShift GitOps:\nApply consistency across cluster and deployment lifecycles\nConsolidate administration and management of applications across on-premises and cloud environments\nCheck the state of clusters making application constraints known early\nRollback code changes across clusters\nRoll out new changes submitted via Git\nCreate an Amazon DynamoDB Instance # From the OpenShift Console Administrator view click through HOME -\u0026gt; Operators -\u0026gt; Operator Hub, search for \u0026ldquo;openshift gitops\u0026rdquo; and click Install. For the update channel select gitops-1.11. Leave all other defaults and click Install.\nWait until the operator shows as successfully installed (Installed operator - ready for use).\nIn your bastion VM create a new project\noc new-project bgd Sample Output\nNow using project \u0026#34;bgd\u0026#34; on server \u0026#34;https://api.rosa-6n4s8.1c1c.p1.openshiftapps.com:6443\u0026#34;. You can add applications to this project with the \u0026#39;new-app\u0026#39; command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname Deploy ArgoCD into your project\ncat \u0026lt;\u0026lt;EOF | oc apply -f - --- apiVersion: argoproj.io/v1beta1 kind: ArgoCD metadata: name: argocd namespace: bgd spec: sso: dex: openShiftOAuth: true resources: limits: cpu: 500m memory: 256Mi requests: cpu: 250m memory: 128Mi provider: dex rbac: defaultPolicy: \u0026quot;role:readonly\u0026quot; policy: \u0026quot;g, system:authenticated, role:admin\u0026quot; scopes: \u0026quot;[groups]\u0026quot; server: insecure: true route: enabled: true tls: insecureEdgeTerminationPolicy: Redirect termination: edge EOF Sample Output\nargocd.argoproj.io/argocd created Wait for ArgoCD to be ready\noc rollout status deploy/argocd-server -n bgd Sample Output\ndeployment \u0026#34;argocd-server\u0026#34; successfully rolled out Apply the GitOps application for your application\ncat \u0026lt;\u0026lt;EOF | oc apply -f - --- apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bgd-app namespace: bgd spec: destination: namespace: bgd server: https://kubernetes.default.svc project: default source: path: apps/bgd/base repoURL: https://github.com/rh-mobb/gitops-bgd-app targetRevision: main syncPolicy: automated: prune: true selfHeal: false syncOptions: - CreateNamespace=false EOF Sample Output\napplication.argoproj.io/bgd-app created Find the URL for your Argo CD dashboard, copy it to your web browser and log in using your OpenShift credentials (remind yourself what your password for user admin is by typing echo $COGNITO_ADMIN_PASSWORD - {ssh_password}-2@23 if you accepted the default)\noc get route argocd-server -n bgd -o jsonpath='{.spec.host}{\u0026quot;\\n\u0026quot;}' Sample Output\nargocd-server-bgd.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com Click on the Application to show its topology\nVerify that OpenShift sees the Deployment as rolled out\noc rollout status deploy/bgd Sample Output\ndeployment \u0026#34;bgd\u0026#34; successfully rolled out Get the route and browse to it in your browser\noc get route bgd -n bgd -o jsonpath='{.spec.host}{\u0026quot;\\n\u0026quot;}' Sample Output\nbgd-bgd.apps.rosa-6n4s8.1c1c.p1.openshiftapps.com You should see a green box in the website\nPatch the OpenShift resource to force it to be out of sync with the github repository. To do this we’re going to turn the tile blue by changing the deployed code directly. This means the value set in the deployed code (\u0026ldquo;blue\u0026rdquo;) and the value in the repo (\u0026ldquo;green\u0026rdquo;) are now different\noc -n bgd patch deploy/bgd --type='json' \\ -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/template/spec/containers/0/env/0/value\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;blue\u0026quot;}]' Sample Output\ndeployment.apps/bgd patched Refresh Your browser and you should see a blue box in the website\nMeanwhile check ArgoCD it should show the application as out of sync. Click the Sync button and then click on Synchronize to have it revert the change you made directly to the code deployed in OpenShift\nCheck again, you should see a green box in the website\nCongratulations! You have successfully deployed OpenShift Gitops\n"},{"id":10,"href":"/docs/lab_11_network_policy/","title":"Lab 11 - Secure your applications with Network Policies","section":"Docs","content":" Introduction # NetworkPolicies are used to control and secure communication between pods within a cluster. They provide a declarative approach to define and enforce network traffic rules, allowing you to specify the desired network behavior. By using NetworkPolicies, you can enhance the overall security of your applications by isolating and segmenting different components within the cluster. These policies enable fine-grained control over network access, allowing you to define ingress and egress rules based on criteria such as IP addresses, ports, and pod selectors.\nFor this module we will be applying networkpolices to the previously created \u0026lsquo;microsweeper-ex\u0026rsquo; namespace and using the \u0026lsquo;microsweeper\u0026rsquo; app to test these policies. In addition, we will deploy two new applications to test against the \u0026lsquo;microsweeper\u0026rsquo; app\nCreate a new project and a new app. We will be using this pod for testing network connectivity to the microsweeper application\noc new-project networkpolicy-test Create a new application within this namespace\ncat \u0026lt;\u0026lt; EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: networkpolicy-pod namespace: networkpolicy-test labels: app: networkpolicy spec: securityContext: allowPrivilegeEscalation: false containers: - name: networkpolicy-pod image: registry.access.redhat.com/ubi9/ubi-minimal command: [\u0026quot;sleep\u0026quot;, \u0026quot;infinity\u0026quot;] EOF Now we will change to the microsweeper-ex project to start applying the network policies\noc project microsweeper-ex Fetch the IP address of the microsweeper Pod\nMS_IP=$(oc -n microsweeper-ex get pod -l \\ \u0026quot;app.kubernetes.io/name=microsweeper-appservice\u0026quot; \\ -o jsonpath=\u0026quot;{.items[0].status.podIP}\u0026quot;) echo $MS_IP Sample Output\n10.128.2.242 Check to see if the networkpolicy-pod can access the microsweeper pod.\noc -n networkpolicy-test exec -ti pod/networkpolicy-pod -- curl $MS_IP:8080 | head Sample Output\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Microsweeper\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;css/main.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.2.1.min.js\u0026#34; It’s common to want to prohibit Pods from accessing each other between projects (namespaces). This can be done with a fairly simple Network Policy.\nThis Network Policy will restrict Ingress to the Pods in the project microsweeper-ex to just the OpenShift Ingress Pods and only on port 8080.\ncat \u0026lt;\u0026lt; EOF | oc apply -f - --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-from-openshift-ingress namespace: microsweeper-ex spec: ingress: - from: - namespaceSelector: matchLabels: network.openshift.io/policy-group: ingress podSelector: {} policyTypes: - Ingress EOF Sample Output\nnetworkpolicy.networking.k8s.io/allow-from-openshift-ingress created Try to access microsweeper from the networkpolicy-pod again\noc -n networkpolicy-test exec -ti pod/networkpolicy-pod -- curl $MS_IP:8080 | head This time it should fail to connect - it will just sit there. Hit Ctrl-C to avoid having to wait until a timeout.\nTip: If you have your browser still open to the microsweeper app, you can refresh and see that you can still access it. Sometimes you want your application to be accessible to other namespaces. You can allow access to just your microsweeper frontend from the networkpolicy-pod in the networkpolicy-test namespace like so\ncat \u0026lt;\u0026lt;EOF | oc apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-networkpolicy-pod-ap namespace: microsweeper-ex spec: podSelector: matchLabels: app.kubernetes.io/name: microsweeper-appservice ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: networkpolicy-test podSelector: matchLabels: app: networkpolicy EOF Sample Output\nnetworkpolicy.networking.k8s.io/allow-networkpolicy-pod-ap created Check to see if networkpolicy-pod can access the Pod.\noc -n networkpolicy-test exec -ti pod/networkpolicy-pod -- curl $MS_IP:8080 | head Sample Output\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Microsweeper\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;css/main.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.2.1.min.js\u0026#34; To verify that only the networkpolicy-pod app can access the microsweeper app, create a new pod with a different label in the networkpolicy-test namespace.\ncat \u0026lt;\u0026lt; EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: new-test namespace: networkpolicy-test labels: app: new-test spec: securityContext: allowPrivilegeEscalation: false containers: - name: new-test image: registry.access.redhat.com/ubi9/ubi-minimal command: [\u0026quot;sleep\u0026quot;, \u0026quot;infinity\u0026quot;] EOF Try to curl the microsweeper-ex pod from our new pod.\noc -n networkpolicy-test exec -ti pod/new-test -- curl $MS_IP:8080 | head This will fail with a timeout again. Hit Ctrl-C to avoid waiting for a timeout.\nFor information on setting default network policies for new projects you can read the OpenShift documentation on modifying the default project template.\nSummary # Here’s what you learned:\nNetwork Policies are a powerful way to apply zero-trust networking patterns.\nAccess to pods can be restricted to other Pods, Namespaces, or other labels.\nAccess can be completely denied, allowed, or set to particular ports or services.\n"},{"id":11,"href":"/docs/lab_12_resilient_app/","title":"Lab 12 - Make your application resilient","section":"Docs","content":" Introduction # ROSA is designed with high availability and resiliency in mind. Within ROSA there are multiple tools at your disposal to leverage this highly available and resilient architecture to ensure maximum uptime and availability for your applications. Disruptions can occur for a variety of different reasons, but with proper configuration of the application, you can eliminate application disruption.\nLimits and Requests can be used to both allocate and restrict the amount of resources an application can use, pod disruption budgets ensure that you always have a particular number of your application pods running and the Horizontal Pod Autoscaler can automatically increase and decrease pod count as needed.\nIn this section of the workshop, we will use the previously deployed microsweeper application, ensure the application is resilient to node failure, and scale the application when under load.\nSetting Limits and Requests on an Application # First, let’s set limits and requests on the previously deployed microsweeper application.\noc -n microsweeper-ex set resources deployment/microsweeper-appservice \\ --limits=cpu=60m,memory=250Mi \\ --requests=cpu=50m,memory=200Mi Requests state the minimum CPU and memory requirements for a container. This will ensure that the pod is placed on a node that can meet those requirements. Limits set the maximum amount of CPU and Memory that can be consumed by a container and ensure that a whole container does not consume all of the resources on a node. Setting limits and requests for deployments is best practice for resource management and ensuring the stability and reliability of your applications.\noc adm top pods -n microsweeper-ex Sample Output\nNAME CPU(cores) MEMORY(bytes) microsweeper-appservice-65799476b6-vh9q7 0m 191Mi Now that we’ve updated the resource, we can see that a new pod was automatically rolled out with these new limits and requests. To do so, run the following command.\noc get pods -n microsweeper-ex We’ll see a new pod (created 28 seconds earlier in this case):\nSample Output\n[user0_mobbws@bastion ~]$ oc get pods microsweeper-ex NAME READY STATUS RESTARTS AGE microsweeper-appservice-1-build 0/1 Completed 0 17h microsweeper-appservice-69596ccc54-fsw2b 1/1 Running 0 28s To see what the limits and requests added to the pod, run the following command:\noc get pods -l app.kubernetes.io/name=microsweeper-appservice \\ -o yaml -n microsweeper-ex | grep limits -A5 Sample Output\nlimits: cpu: 60m memory: 250Mi requests: cpu: 50m memory: 200Mi We can now use the route of the application to ensure the application is functioning with the new limits and requests. To get the route, run the following command\noc -n microsweeper-ex get route microsweeper-appservice \\ -o jsonpath='http://{.spec.host}{\u0026quot;\\n\u0026quot;}' Then visit the URL presented in a new tab in your web browser (using HTTP). For example, your output will look something similar to:\nSample Output\nhttp://microsweeper-appservice-microsweeper-ex.apps.test-cluster.2ubs.p1.openshiftapps.com In that case, you’d visit http://microsweeper-appservice-microsweeper-ex.apps.test-cluster.2ubs.p1.openshiftapps.com in your browser.\nInitially, this application is deployed with only one pod. In the event a worker node goes down or the pod crashes, there will be an outage of the application. To prevent that, let’s scale the number of instances of our applications up to three. To do so, run the following command\noc -n microsweeper-ex scale deployment \\ microsweeper-appservice --replicas=3 Next, let’s check to see that the application has scaled. To do so, run the following command to see the pods\noc -n microsweeper-ex get pods Sample Output\nNAME READY STATUS RESTARTS AGE microsweeper-appservice-1-build 0/1 Completed 0 18h microsweeper-appservice-69596ccc54-6lstj 1/1 Running 0 2m41s microsweeper-appservice-69596ccc54-fsw2b 1/1 Running 0 39m microsweeper-appservice-69596ccc54-rkpgj 1/1 Running 0 2m41s In addition you can see the number of pods, how many are on the current version, and how many are available by running the following\noc -n microsweeper-ex get deployment microsweeper-appservice Sample Output\nNAME READY UP-TO-DATE AVAILABLE AGE microsweeper-appservice 3/3 3 3 18h Pod Disruption Budget # A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for upgrades or routine maintenance work on ROSA nodes. In essence, it lets developers define the minimum tolerable operational requirements for a deployment so that it remains stable even during a disruption.\nFor example, microsweeper-appservice deployed as part of the last step contains three replicas distributed evenly across three nodes. We can tolerate losing one pod, so we create a PDB that requires a minimum of one replica.\nA PodDisruptionBudget object’s configuration consists of the following key parts:\nA label selector, which is a label query over a set of pods.\nAn availability level, which specifies the minimum number of pods that must be available simultaneously, either:\nminAvailable is the number of pods that must always be available, even during a disruption. maxUnavailable is the number of pods that can be unavailable during a disruption. Warning: A maxUnavailable of 0% or 0 or a minAvailable of 100% or equal to the number of replicas can be used but will block nodes from being drained and can result in application instability during maintenance activities.\nLet’s create a Pod Disruption Budget for our microsweeper-appservice application. To do so, run the following command.\ncat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: microsweeper-appservice-pdb namespace: microsweeper-ex spec: minAvailable: 1 selector: matchLabels: deployment: microsweeper-appservice EOF After creating the PDB, the OpenShift API will ensure at least one pod of microsweeper-appservice is running all the time, even when maintenance is going on within the cluster.\nNext, let’s check the status of Pod Disruption Budget. To do so, run the following command\noc -n microsweeper-ex get poddisruptionbudgets Sample Output\nNAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE microsweeper-appservice-pdb 1 N/A 0 39s Horizontal Pod Autoscaler # As a developer, you can utilize a horizontal pod autoscaler (HPA) in ROSA clusters to automate scaling of replication controllers or deployment configurations. The HPA adjusts the scale based on metrics gathered from the associated pods. It is applicable to deployments, replica sets, replication controllers, and stateful sets.\nThe HPA (Horizontal Pod Autoscaler) provides you with automated scaling capabilities, optimizing resource management and improving application performance. By leveraging an HPA, you can ensure your applications dynamically scale up or down based on workload. This automation reduces the manual effort of adjusting application scale and ensures efficient resource utilization, by only using resources that are needed at a certain time. Additionally, the HPA’s ease of configuration and compatibility with various workload types make it a flexible and scalable solution for developers in managing their applications.\nIn this exercise we will scale the microsweeper-appservice application based on CPU utilization:\nScale out when average CPU utilization is greater than 50% of CPU limit\nMaximum pods is 4\nScale down to min replicas if utilization is lower than threshold for 60 sec\nFirst, we should create the HorizontalPodAutoscaler. To do so, run the following command\ncat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: microsweeper-appservice-cpu namespace: microsweeper-ex spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: microsweeper-appservice minReplicas: 2 maxReplicas: 4 metrics: - type: Resource resource: name: cpu target: averageUtilization: 50 type: Utilization behavior: scaleDown: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 15 EOF Next, check the status of the HPA. To do so, run the following command\noc -n microsweeper-ex get horizontalpodautoscaler/microsweeper-appservice-cpu Sample Output\nNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE microsweeper-appservice-cpu Deployment/microsweeper-appservice 0%/50% 2 4 3 43s Next, let’s generate some load against the microsweeper-appservice application. To do so, run the following command\nFRONTEND_URL=http://$(oc -n microsweeper-ex get route microsweeper-appservice -o jsonpath='{.spec.host}')/ ab -c100 -n10000 ${FRONTEND_URL} Apache Bench will take around 100 seconds to complete (you can also hit CTRL-C to kill the ab command). Then immediately check the status of Horizontal Pod Autoscaler. To do so, run the following command:\noc -n microsweeper-ex get horizontalpodautoscaler/microsweeper-appservice-cpu Sample Output\nNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE microsweeper-appservice-cpu Deployment/microsweeper-appservice 135%/50% 2 4 4 7m37s This means you are now running 4 replicas, instead of the original three that we started with.\nOnce you’ve killed the ab command, the traffic going to microsweeper-appservice service will cool down and after a 60 second cool down period, your application’s replica count will drop back down to two. To demonstrate this, run the following command\noc -n microsweeper-ex get horizontalpodautoscaler/microsweeper-appservice-cpu --watch After a minute or two, your output should be similar to below\nNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE microsweeper-appservice-cpu Deployment/microsweeper-appservice 0%/50% 2 4 4 19m microsweeper-appservice-cpu Deployment/microsweeper-appservice 0%/50% 2 4 4 19m microsweeper-appservice-cpu Deployment/microsweeper-appservice 0%/50% 2 4 2 20m Summary # Here’s what you learned:\nSet Limits and Requests on the Microsweeper application from the previous section\nScale the Microsweeper application up and down\nSet a Pod Disruption Budget on the Microsweeper application\nSet a Horizontal Pod Autoscaler to automatically scale application based on load.\n"}]